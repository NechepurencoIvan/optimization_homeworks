\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Домашнее задание № 1}
\author{Иван Нечепуренко }
\date{September 2018}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage[english,russian]{babel}	% локализация и переносы

\begin{document}

\maketitle

\section{Посели меня, если сможешь}
Прежде всего, введем исходный вектор 
$x = (a_{1,1}, .. , a_{n, m})$, $a_{i, k} \in  \{0, 1\} $, где каждая переменная
$a_{i, j} $ - индикатор того, то i-й студент поселен в k-ю комнату. Тогда можно 
указать минимализируемую функцию: это $-(\sum\limits_{i, k}a_{i, k}*b_{i, k} + \sum\limits_{i, j, k}a_{i, k}*a_{j, k}*p_{i, j})$. Здесь (во второй сумме) умножение индикаторов очень удобно перерастает в логическое умножение. Далее, первое ограничение - это то, что каждый студент должен быть заселен:
$\sum\limits_{k = 1}^{m}a_{i, k} = 1, i = 1 .. n$. Также вводится ограничение на 
колличество студентов в одной комнате: $\sum\limits_{i = 1}^{n}a_{i, k} < 3, k = 1 .. m$. Эти ограничения достаточны в рамках нашей задачи. Что касается недостающих 
данных, то самый очевидный вариант - занулить все неизвестные переменные - достаточно хорош, если все знаения положительны. В таком случае мы имеем пессимистическую оценку: решение задачи даст нам гарантированный, хотя, возможно, не
самый лучший вариант. Иначе можно пробовать делать какие-то предсказания, усреднять
коэффициенты - задача творческая, но очень сильно опирающаяся на необговоренную модель.

\section{Перевозчик}

 Пусть на i-ом складе сети N располагается ai единиц товара, который необходимо развезти по m магазинам сети. Каждому j-ому магазину необходимо bj единиц товара. Перевозка единицы товара из i-го склада в j-ый магазин стоит cij и занимает tij единиц времени. Сколько единиц товара с каждого склада нужно вывезти, чтобы обеспечить товаром все
магазины сети?

Это - именна та задача, из-за формулировки которой могут возникуть недопонимания
между заказчиком и работником. Вектор x почти очевиден:
$x = (x_{1,1}, .. , x_{n, m})$, n - кол-во складов, m - магазинов, $x_{i, j}$ - кол-во перевозимого из i-го склада в j-й магазин товара, скорее всего, целое число. Очевидна также пара ограничений:
$\sum\limits_{i = 1}^{n}x_{i, j} = b_j, j = 1 .. m$, 
$\sum\limits_{j = 1}^{m}x_{i, j} \leq a_i, i = 1 .. n$. А вот что дальше требуется от нас, не совсем понятно. Могут быть функции для минимализации: 
суммарная стоимость ($\sum\limits_{i, j}x_{i, j}c_{i, j} $), максимальное 
время доставки ($ max(t_{i, j}*Id(x_{i,j} \neq 0))$), или некоторая взвешенная сумма двух предыдущих функций. Или, наоборот, мы имеем ограниченное число времени/денег,
и тогда предыдущие функции уже выступают в роли ограничений

\section{Это норма!}

$\; \; \; \; \;$1) Норма в векторном пространстве $P$ есть функция $L \to R_+$, удовлетворяющая следующим условиям: \\
$||x|| = 0$ только при $x = 0$;\\
$||x+y|| < ||x||+||y||$ для всех $x, y \in P$ (неравенство треугольника);\\
$||\alpha\, x||=|\alpha|*||x||$ для любого скаляра $\alpha$.\\

Нормы $||x||_1, ||x||_2$ называются эквивалентными, если 
$\exists c_1 > 0, c_2 > 0$, $\forall x \; c_1||x||_1 \leq ||x||_2 \leq c_2||x||_1 $.
Если переписать это соотношение в виде 
$c_1||x||_1 \leq ||x||_2, $
$c_2||x||_1 \geq ||x||_2 $, то становится довольно очевидно утверждение, что мы имеем дело действительно с отношением эквивалентности. 

Положим $x = (x_1, .. , x_n)$. Тогда: \\
$l_1(x) = |x_1| + .. |x_n|$\\
$l_2(x) = \sqrt{(x_1)^2 + .. (x_n)^2}$\\
$l_\infty(x) = max_{i = 1 .. n}(|x_i|)$\\

Из утверждения об эквивалентности нам достаточно доказать только эквивалентности
первых двух норм третьей. Для $l_1:$\\
$l_\infty(x) = max_{i = 1 .. n}(|x_i|) \leq |x_1| + .. |x_n| = l_1(x) \leq
n*max_{i = 1 .. n}(|x_i|) = n*l_\infty(x)$

Для $l_2$ и $l_\infty$ имеем:\\
$l_\infty(x) = max_{i = 1 .. n}(|x_i|) = \sqrt{max_{i = 1 .. n}(|x_i|)^2} \leq 
\sqrt{(x_1)^2 + .. (x_n)^2} = l_2(x) = \sqrt{(x_1 + .. + x_n)^2} \leq 
\sqrt{(n*max_{i = 1 .. n}(|x_i|))^2} = n*max_{i = 1 .. n}(|x_i|) = n*l_\infty(x)$\\
Эквивалентность этих норм доказана.

2) Норма матрицы $A$, порожденная векторной нормой, задается выраженикем 
$\sup_{x \neq 0}\frac{||Ax||}{||x||}$.)(или по другому?). Из элементарных
соображений линейной алгебры следует, что нам достаточно рассматривать только
векторы с единичной нормой(нормирование вектора ничего не меняет).\\ 

Рассмотрим $L_{\inf}$. $\exists x, ||x|| = 1$
$L_{\inf}(A) = \max\limits_{1\leq i \leq m } \left| \sum\limits_{j = 1}^{n}a_{i, j}x_j\right|  \leq 
\max\limits_{1\leq i \leq m } \left| \sum\limits_{j = 1}^{n}a_{i, j}\right|$
($ |x_j| \leq ||x|| = 1$)
, а т. к. мы всегда можем взять $|x_j| = 1, sign(x_j) = sign(a_{i, j})$, то подстановкой уюеждаемся, что верхнее ограничение можно достигнуть, и 
$L_{\inf}(A) = \max\limits_{1\leq i \leq m } \left| \sum\limits_{j = 1}^{n}a_{i, j}\right|$

Теперь перейдем к $L_1$.  $\exists x, ||x|| = 1, L_1(A) = ||Ax||_1$
$
L_{1}(A) = \sum\limits_{i = 1}^{m} \left| \sum\limits_{j = 1}^{n} a_{i, j}x_j \right|
\leq
\sum\limits_{i = 1}^{m} \sum\limits_{j = 1}^{n} |a_{i, j}| |x_j|
=
\sum\limits_{j = 1}^{n} |x_j| \sum\limits_{i = 1}^{m} |a_{i, j}|
\leq
||x||_1 \max\limits_{j = 1 .. n} \sum\limits_{i = 1}^{m} |a_{i, j}|
=
\max\limits_{j = 1 .. n} \sum\limits_{i = 1}^{m}  |a_{i, j}|
$ \\

Но с другой стороны, нетрудно понять, что данная оценка достигается на векторе 
$x: x_j = 1, x_i = 0, i \neq j, j = \arg(\max\limits_{j = 1 .. n} \sum\limits_{i = 1}^{m} |a_{i, j}|)$. Значит, мы имеем $L_{1}(A) = \max\limits_{j = 1 .. n} \sum\limits_{i = 1}^{m}  |a_{i, j}|$//

Перейдем же, наконец, к $L_2$. Для этого воспользуемся
сингулярным разложением: $A = UJV^{T}$, $UU^{T} = E, VV^{T} = E, 
J = diag(\sigma_1, .. , \sigma_n)$. Иными словами, матрицы  $U, V$ - ортогональные,
а значит, сохраняют скалярное произведение. В частности, матрица $V$ - обратима, а это значит, что множества векторов $x, x \neq 0$ и $Vx, Vx \neq 0$, совпадают. Тогда можем выписать такую цепочку: $
||A||^2_2 = \sup\limits_{x \neq 0} \frac{(Ax, Ax)}{(x, x)} = 
 \sup\limits_{Vx \neq 0} \frac{(UJV^TVx, UJV^TVx)}{(Vx, Vx)} =
 \sup\limits_{Vx \neq 0} \frac{(Jx, Jx)}{(x, x)} =
 \sup\limits_{x \neq 0} \frac{(Jx, Jx)}{(x, x)}
$
В последнем равенстве мы пользовались, опять же, невырожденностью $V$, следующей 
из обратимости. Возьмем $||x|| = 1$, как и в предыдущих пунктах.
Мы видим, что оптимальный для нас вектор $x: x_j = 1, x_i = 0, i \neq j $, где 
$ j = arg(\max\limits_{i = 1, .., n}(|\sigma_i|))$, и на нем достигается масимальное
значение $(\max\limits_{i = 1, .., n}(|\sigma_i|))^2$. Это - ничто иное, как максимальное
сингулярное число. В то же время для симметричной матрицы это модуль максимального собственного значения. //
Что касается Фробениусовой нормы, она является средним квадратическим всех элементов матрицы, и очевидна аналогия с $l_2$.

\section{Вычисли это}
\section{Диаграмма Вороного}
\section{Выпуклый или конический}

\end{document}
